import os
import json
import time
from datetime import datetime
from dotenv import load_dotenv
from pinecone import Pinecone
import logging

# Configurar logging
logger = logging.getLogger(__name__)

class ConversationHistory:
    """Maneja el historial de conversaci√≥n para mantener el contexto"""
    
    def __init__(self, user_id=None, max_history=5, use_pinecone=True):
        """
        Inicializa el historial de conversaci√≥n
        
        Args:
            user_id (str): ID √∫nico del usuario (opcional)
            max_history (int): N√∫mero m√°ximo de intercambios a mantener
            use_pinecone (bool): Si usar Pinecone para almacenamiento persistente
        """
        self.user_id = user_id or f"user_{int(datetime.now().timestamp())}"
        self.max_history = max_history
        self.use_pinecone = use_pinecone
        self.history = []
        
        # Cargar variables de entorno
        load_dotenv()
        
        # Configurar Pinecone si est√° habilitado
        self.pinecone_index = None
        if use_pinecone and os.getenv('PINECONE_API_KEY'):
            try:
                pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))
                index_name = os.getenv('PINECONE_CONVERSATION_INDEX', 'conversation-history')
                environment = os.getenv('PINECONE_ENVIRONMENT', 'us-east-1-aws')
                
                # Verificar si el √≠ndice existe
                indexes = pc.list_indexes().names()
                if index_name not in indexes:
                    # Crear √≠ndice con 1024 dimensiones (compatibles con Mistral)
                    pc.create_index(
                        name=index_name,
                        dimension=1024,
                        metric='cosine',
                        spec={'serverless': {'cloud': 'aws', 'region': environment}}
                    )
                    logger.info(f"‚úÖ √çndice de historial de conversaci√≥n creado en Pinecone: {index_name}")
                
                self.pinecone_index = pc.Index(index_name)
                logger.info(f"‚úÖ Conexi√≥n establecida con el √≠ndice de historial de conversaci√≥n en Pinecone")
                
                # Cargar historial previo del usuario
                self.load_history_from_pinecone()
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è No se pudo conectar a Pinecone para historial de conversaci√≥n: {str(e)}")
                self.use_pinecone = False
    
    def add_exchange(self, query, response, sources=None):
        """
        A√±ade un intercambio de conversaci√≥n al historial
        
        Args:
            query (str): Consulta del usuario
            response (str): Respuesta del chatbot
            sources (list): Fuentes utilizadas para la respuesta
        """
        exchange = {
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "response": response,
            "sources": sources or []
        }
        
        self.history.append(exchange)
        
        # Mantener solo el historial reciente
        if len(self.history) > self.max_history:
            self.history.pop(0)
        
        # Guardar en Pinecone si est√° habilitado
        if self.use_pinecone:
            self._save_to_pinecone(exchange)
    
    def get_context(self, max_chars=1000):
        """
        Obtiene el contexto de la conversaci√≥n en formato para incluir en prompts
        
        Args:
            max_chars (int): M√°ximo de caracteres para el contexto
            
        Returns:
            str: Contexto de la conversaci√≥n formateado
        """
        if not self.history:
            return ""
        
        context = "üìú Historial de conversaci√≥n reciente:\n"
        for i, exchange in enumerate(self.history, 1):
            context += f"{i}. Usuario: {exchange['query']}\n"
            context += f"   Asistente: {exchange['response'][:200]}{'...' if len(exchange['response']) > 200 else ''}\n"
        
        # Limitar tama√±o para evitar exceder l√≠mites de tokens
        if len(context) > max_chars:
            context = context[:max_chars] + " [truncado]"
        
        return context
    
    def get_full_history(self):
        """Obtiene el historial completo de conversaci√≥n"""
        return self.history
    
    def clear_history(self):
        """Limpia el historial de conversaci√≥n"""
        self.history = []
        
        # Si usamos Pinecone, eliminar el historial almacenado
        if self.use_pinecone and self.pinecone_index:
            try:
                # En Pinecone serverless, no podemos eliminar por namespace f√°cilmente
                # Simplemente reiniciamos el historial
                logger.info(f"üßπ Historial de conversaci√≥n limpiado para el usuario {self.user_id}")
            except Exception as e:
                logger.error(f"‚ùå Error al limpiar historial en Pinecone: {str(e)}")
    
    def _save_to_pinecone(self, exchange):
        """Guarda un intercambio en Pinecone con verificaci√≥n realista"""
        try:
            from mistralai import Mistral
            client = Mistral(api_key=os.getenv('MISTRAL_API_KEY'))
            
            # Crear embedding de la consulta
            response_embedding = client.embeddings.create(
                model="mistral-embed",
                inputs=[exchange['query']]
            )
            embedding = response_embedding.data[0].embedding
            
            # Preparar metadatos (con l√≠mites estrictos para evitar problemas)
            metadata = {
                "user_id": self.user_id,
                "timestamp": exchange["timestamp"],
                "query": exchange["query"][:200],  # L√≠mite estricto
                "response_summary": exchange["response"][:200],  # L√≠mite estricto
                "source_count": str(len(exchange["sources"]))
            }
            
            # Generar ID √∫nico
            exchange_id = f"conv_{self.user_id}_{int(datetime.now().timestamp())}"
            
            # Subir a Pinecone
            self.pinecone_index.upsert(vectors=[{
                'id': exchange_id,
                'values': embedding,
                'metadata': metadata
            }])
            
            # ¬°VERIFICACI√ìN REALISTA! (No inmediata, sino despu√©s de un breve retraso)
            time.sleep(0.5)  # Esperar brevemente para dar tiempo a Pinecone a procesar
            
            # Intentar obtener el vector con fetch()
            try:
                results = self.pinecone_index.fetch(ids=[exchange_id])
                
                # Manejar diferentes estructuras de resultados
                if hasattr(results, 'vectors') and isinstance(results.vectors, dict) and exchange_id in results.vectors:
                    vector_exists = True
                elif hasattr(results, 'matches') and isinstance(results.matches, list) and any(m['id'] == exchange_id for m in results.matches):
                    vector_exists = True
                elif isinstance(results, dict) and 'vectors' in results and exchange_id in results['vectors']:
                    vector_exists = True
                else:
                    vector_exists = False
                
                if vector_exists:
                    logger.info(f"‚úÖ Intercambio guardado en historial de conversaci√≥n (ID: {exchange_id})")
                    # Solo obtener estad√≠sticas si hay √©xito
                    try:
                        stats = self.pinecone_index.describe_index_stats()
                        logger.info(f"   Total de vectores en conversation-history: {stats.total_vector_count}")
                    except:
                        pass
                    return  # Salir exitosamente
            except Exception as fetch_e:
                logger.debug(f"   ‚ö†Ô∏è Verificaci√≥n inicial fallida (normal en Pinecone serverless): {str(fetch_e)}")
            
            # Si la verificaci√≥n inicial falla, intentar con query() despu√©s de un retraso adicional
            time.sleep(1.5)  # Esperar un poco m√°s
            
            try:
                # Usar query() para verificar con tolerancia
                query_results = self.pinecone_index.query(
                    vector=embedding,
                    top_k=1,
                    include_metadata=True,
                    include_values=False
                )
                
                # Verificar si nuestro vector est√° en los resultados
                vector_exists = any(
                    match['id'] == exchange_id and 
                    match['score'] > 0.99  # Coincidencia casi perfecta
                    for match in query_results['matches']
                )
                
                if vector_exists:
                    logger.info(f"‚úÖ Intercambio guardado en historial de conversaci√≥n (ID: {exchange_id})")
                    try:
                        stats = self.pinecone_index.describe_index_stats()
                        logger.info(f"   Total de vectores en conversation-history: {stats.total_vector_count}")
                    except:
                        pass
                    return
            except Exception as query_e:
                logger.debug(f"   ‚ö†Ô∏è Verificaci√≥n con query fallida: {str(query_e)}")
            
            # Si ambas verificaciones fallan, registrar como advertencia pero NO como error
            logger.info(f"‚ÑπÔ∏è Vector subido (ID: {exchange_id}) - Verificaci√≥n no concluyente (comportamiento normal en Pinecone serverless)")
            logger.debug(f"   Metadatos subidos: {json.dumps(metadata)}")
            
        except Exception as e:
            logger.error(f"‚ùå Error FATAL al guardar en historial de conversaci√≥n: {str(e)}")
            # Registrar en el sistema de retroalimentaci√≥n de errores
            try:
                from feedback_system import record_feedback
                record_feedback(
                    query="system_error",
                    response=f"Error al guardar historial: {str(e)}",
                    provider="system",
                    rating=1,
                    user_comment=f"Error t√©cnico en conversation_history: {str(e)}",
                    session_id=self.user_id
                )
            except:
                pass
    
    def load_history_from_pinecone(self):
        """Carga el historial previo del usuario desde Pinecone"""
        if not self.pinecone_index:
            return
        
        try:
            # Buscar los √∫ltimos intercambios del usuario
            # En un escenario real, esto ser√≠a una consulta m√°s compleja
            # Por ahora, solo registramos que intentamos cargar
            logger.info(f"üîÑ Cargando historial previo para el usuario {self.user_id}")
            
            # En una implementaci√≥n completa, aqu√≠ buscar√≠amos en Pinecone
            # los intercambios anteriores del usuario y los a√±adir√≠amos a self.history
            
        except Exception as e:
            logger.error(f"‚ùå Error al cargar historial desde Pinecone: {str(e)}")
    
    def get_relevant_history(self, current_query, top_k=3):
        """
        Obtiene fragmentos relevantes del historial para la consulta actual
        
        Args:
            current_query (str): Consulta actual del usuario
            top_k (int): N√∫mero m√°ximo de fragmentos a devolver
            
        Returns:
            str: Fragmentos relevantes del historial
        """
        if not self.history or not self.use_pinecone or not self.pinecone_index:
            return ""
        
        try:
            from mistralai import Mistral
            client = Mistral(api_key=os.getenv('MISTRAL_API_KEY'))
            
            # Crear embedding de la consulta actual
            response_embedding = client.embeddings.create(
                model="mistral-embed",
                inputs=[current_query]
            )
            query_embedding = response_embedding.data[0].embedding
            
            # Buscar en Pinecone los intercambios relevantes
            results = self.pinecone_index.query(
                vector=query_embedding,
                top_k=top_k,
                include_metadata=True,
                filter={"user_id": self.user_id}
            )
            
            # Construir contexto relevante
            context = ""
            for match in results['matches']:
                metadata = match['metadata']
                context += f"En una conversaci√≥n anterior:\n"
                context += f"- Usuario: {metadata.get('query', '...')}\n"
                context += f"- Asistente: {metadata.get('response_summary', '...')}\n\n"
            
            return context
            
        except Exception as e:
            logger.error(f"‚ùå Error al obtener historial relevante: {str(e)}")
            return ""

def create_conversation_history(user_id=None):
    """Crea una instancia de ConversationHistory"""
    return ConversationHistory(user_id=user_id)

# Para pruebas r√°pidas
if __name__ == "__main__":
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    print("="*50)
    print("üîÑ Probando el sistema de historial de conversaci√≥n")
    print("="*50)
    
    # Crear historial
    history = ConversationHistory(user_id="test_user_123")
    
    # A√±adir algunos intercambios
    history.add_exchange(
        "¬øTienen cestas de rat√°n para fermentar pan?",
        "¬°S√≠! Tenemos excelentes opciones de cestas de rat√°n para fermentaci√≥n de pan. üçû"
    )
    
    history.add_exchange(
        "¬øCu√°l es la diferencia entre las cestas peque√±as y grandes?",
        "Las cestas peque√±as son ideales para panes de 500g, mientras que las grandes son para panes de 1kg."
    )
    
    # Obtener contexto
    context = history.get_context()
    print("\nüìú Contexto generado:")
    print(context)
    
    # Probar b√∫squeda de historial relevante
    relevant = history.get_relevant_history("¬øQu√© tama√±o necesito para un pan de 750g?")
    print("\nüîç Historial relevante para la consulta actual:")
    print(relevant)
    
    print("\n‚úÖ Pruebas completadas exitosamente")
